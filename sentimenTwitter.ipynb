{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oz2xyu5Rsci"
   },
   "source": [
    "# Machine Learning Project\n",
    "\n",
    "### T2 - Sentiment Analysis: Twitter\n",
    "The aim of this project is to develop a neural network to perform sentiment analysis tasks for twitter tweets. We will try different models to show which ones perform best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U \"tensorflow==2.14.0\"\n",
    "#!pip install -U \"tensorflow-hub==0.14.0\"\n",
    "#!pip install -U \"tensorflow-text==2.14.0\"\n",
    "#!pip install \"tf-models-official==2.14.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJLpTvWmTQRC"
   },
   "source": [
    "## Import libraries and models\n",
    "We import the libraries and models that we will need for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6MMYoPSLUuGr",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'official'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhub\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_text\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtext\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mofficial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnlp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimization\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, TFAutoModelForSequenceClassification\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'official'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import spacy\n",
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS']='1'\n",
    "import shutil\n",
    "import string\n",
    "import nltk\n",
    "import warnings\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tensorflow import keras\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import Model, Input\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Choose a BERT model to fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjVwhJnAj9Bx"
   },
   "outputs": [],
   "source": [
    "bert_model_name = 'small_bert/bert_en_uncased_L-2_H-128_A-2'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "#these are dictionary that act like lookup tables\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GF787_PRlN_d"
   },
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGnE9D7OT4C7"
   },
   "source": [
    "## Load the dataset\n",
    "Here we extract all the content from the TwitterParsed.zip archive, then we load all the tweets (both positive and negative) and save them in a single file called 'sentiment_data.csv'. In this way, we can manage the data more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jGmKa7VnWmgL",
    "outputId": "ce16ab59-31f4-4c81-ec14-6f3b81564390"
   },
   "outputs": [],
   "source": [
    "# 1. Zip file path\n",
    "zip_path = 'TwitterParsed.zip'\n",
    "extract_path = 'estrazione'\n",
    "\n",
    "# Extract all the content from the zip file\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "# 2. Setting folder paths\n",
    "positive_path = os.path.join(extract_path, 'TwitterParsed/1')\n",
    "negative_path = os.path.join(extract_path, 'TwitterParsed/0')\n",
    "\n",
    "# Reading tweets and building DataFrame\n",
    "def load_data_from_folder(folder_path, sentiment_label):\n",
    "    data = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read().strip()  # Read and remove blank spaces\n",
    "            data.append({'text': text, 'sentiment': sentiment_label})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# 3. Load positive and negative data\n",
    "positive_data = load_data_from_folder(positive_path, 1)  # Label 1 for positives\n",
    "negative_data = load_data_from_folder(negative_path, 0)  # Label 0 for negatives\n",
    "\n",
    "# Join the two DataFrames\n",
    "data = pd.concat([positive_data, negative_data], ignore_index=True)\n",
    "\n",
    "# 4. Save the final DataFrame in a single CSV file\n",
    "csv_path = 'sentiment_data.csv'\n",
    "data.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f'Pre-processed data saved in {csv_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_p3q_tO6V16D"
   },
   "source": [
    "With the pd.read_csv() function from pandas, we read the CSV file and load it into a DataFrame, which is a tabular data structure similar to a spreadsheet or a database table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "5_Ovwg7nYkKZ",
    "outputId": "41ec3575-aac7-439a-fa6f-9dad191aac8a"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('sentiment_data.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2KASSdsaRT2"
   },
   "source": [
    "## First Cleaning\n",
    "Here we proceed with an initial data processing by removing all user tags \"@\", links (http/s,www), and citations (&quot) using the str.replace function, which replaces the specified expression with \"\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "9J5VqZw2dw4R",
    "outputId": "37927cd1-2ede-47da-cb91-699036633f41"
   },
   "outputs": [],
   "source": [
    "#remove user tags \"@\"\n",
    "df['clean_tweet'] = df['text'].str.replace(r\"@[\\w]+\", \"\", regex=True)\n",
    "\n",
    "#remove links \"http/s, www\"\n",
    "df['clean_tweet'] = df['clean_tweet'].str.replace(\"http\\S+|https\\S+|www.\\S+\", \"\", case=False, regex=True)\n",
    "\n",
    "#remove quotations \"&quot\"\n",
    "df['clean_tweet'] = df['clean_tweet'].str.replace(\"&quot\", \"\", case=False)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCqXLAqdcHrh"
   },
   "source": [
    "Now we remove all the special characters, the numbers and the punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Z7b1LU91egaW",
    "outputId": "6f08a98c-b265-411a-ccbc-6f427ae8305a"
   },
   "outputs": [],
   "source": [
    "df['clean_tweet'] = df['clean_tweet'].str.replace(\"[^a-zA-Z]\", \" \", regex=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGDtOtaOdHn8"
   },
   "source": [
    "Now we convert all uppercase letters to lowercase.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "74lj7hkLqDd9",
    "outputId": "b228aef3-60cd-4163-f298-f08e004e7701"
   },
   "outputs": [],
   "source": [
    "df['clean_tweet'] = df['clean_tweet'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asttjg5QdjB1"
   },
   "source": [
    "In this phase, we remove all the stop-words and replace each word with its root word using spaCy a library for advanced Natural Language Processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "id": "ySgLx2C-cWQw",
    "outputId": "3d9090c8-fcdf-4b2b-b5e1-7e8753135e6d"
   },
   "outputs": [],
   "source": [
    "#spacy.cli.download(\"en_core_web_sm\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Applica lemmatizzazione e rimuove le stop words\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x: [token.lemma_ for token in nlp(x) if not token.is_stop and len(token.lemma_) > 2])\n",
    "\n",
    "# Visualizza le prime righe\n",
    "df['clean_tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ET-FN1I-ffTL"
   },
   "source": [
    "Now we reassemble all the tokens to form complete sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "id": "DRVYbqYRbQEF",
    "outputId": "eea9f2c6-65f4-4725-debe-42c8220bd7e2"
   },
   "outputs": [],
   "source": [
    "for i in range(len(df['clean_tweet'])):\n",
    "    df['clean_tweet'][i] = ' '.join(df['clean_tweet'][i])\n",
    "\n",
    "\n",
    "df['clean_tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PKrD06Y5rnhU",
    "outputId": "37c244c1-944e-487d-c66b-66f628bbcfd7"
   },
   "outputs": [],
   "source": [
    "#vedo il numero massimo di parole in un tweet\n",
    "max_length = df['clean_tweet'].str.split().str.len().max()\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7NekHAKcE1-",
    "outputId": "56ef5461-0919-4705-93df-c63c8e00217d"
   },
   "outputs": [],
   "source": [
    "#calcolo la lunghezza totale di tutte le frasi\n",
    "total_length = df['clean_tweet'].str.split().str.len().sum()\n",
    "print(total_length)\n",
    "\n",
    "print(len(df['clean_tweet']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vs2t6OXBrO7h"
   },
   "source": [
    "We checked that the dataset doesn't contain empty or duplicated tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TORWq9ambMki"
   },
   "source": [
    "## Plotting\n",
    "Now we plot the most frequent words in positive and negative tweets using the WordCloud library (larger = more frequent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p9FxTgA1KW6w"
   },
   "outputs": [],
   "source": [
    "#plotting the distribution of the classes\n",
    "class_counts = df[\"sentiment\"].value_counts()\n",
    "\n",
    "# Grafico a barre\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x=class_counts.index, y=class_counts.values, palette=[\"red\", \"green\"])\n",
    "plt.xticks([0, 1], [\"Negative (0)\", \"Positive (1)\"])\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of tweets\")\n",
    "plt.title(\"Distirbution of the classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0ZIrh-ELUMv"
   },
   "source": [
    "Since the dataset is balanced, there is no need to use dataset balancing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPeyy05C1aN1"
   },
   "outputs": [],
   "source": [
    "#plotting the most frequent words in the positive tweets using WordCloud\n",
    "all_words = ' '.join([text for text in df['clean_tweet'][df['sentiment'] == 1]])\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=110).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGBGjgbY1fWT"
   },
   "outputs": [],
   "source": [
    "#plotting the most frequent words in the negative tweets using WordCloud\n",
    "all_words = ' '.join([text for text in df['clean_tweet'][df['sentiment'] == 0]])\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=110).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8EepCuhUcte"
   },
   "source": [
    "## Split the dataset: training set, validation set and test set\n",
    "Now we randomly split the dataset. As we know the training set is used for training the models, the validation set is used for calculate the performance of the results and also the test set is used evaluate, at the end, the performance but on new data.\n",
    "'random_state' as the name suggests, is used fot initializing the internal random number generator, wich will decide the splitting of data into training set, validation set and test set. In the documentation is stated that:\n",
    "*If random_state is an integer, then it is used to seed a new RandomState object.*\n",
    "Before of all that we need to separate the data and the label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K-TgIr0ofeFq"
   },
   "outputs": [],
   "source": [
    "#separating the data and the label\n",
    "x = df['clean_tweet']\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dk50SKEGVUp2"
   },
   "outputs": [],
   "source": [
    "#split for test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "\n",
    "#split for validation set\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.15, random_state=42)\n",
    "\n",
    "data_train  = pd.concat([x_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ibkkGr5SnPN7"
   },
   "outputs": [],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nWIuv4YnfiE"
   },
   "outputs": [],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lpeoqtgO00_"
   },
   "source": [
    "## Define the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3-XkCTTqqK3"
   },
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0BAKz89XXNn"
   },
   "source": [
    "Now, we are going to build a model that includes:\n",
    "\n",
    "* Input Layer\n",
    "* Preprocessing Layer\n",
    "* Encoder Layer with BERT\n",
    "* Dropout Layer to improve performance (using keras-tuner to optimize hyperparameters)\n",
    "* Final Output Layer, a fully connected dense layer responsible for the classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IeNgm0J_mjoT"
   },
   "outputs": [],
   "source": [
    "#Define loss function and metrics\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metrics = tf.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18IXx5lsZbtB"
   },
   "outputs": [],
   "source": [
    "def build_TF_model(hp):\n",
    "    \n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    \n",
    "    #setting dropout rate\n",
    "    net = tf.keras.layers.Dropout(hp.Float('dropout_rate',0.1,0.5,step=0.1))(net)\n",
    "        \n",
    "    #setting AdamW\n",
    "    optimizer=tf.keras.optimizers.AdamW(learning_rate=hp.Choice('learning_rate',[1e-5,2e-5,3e-5]), weight_decay=1e-4)\n",
    "    \n",
    "    net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\n",
    "    model = tf.keras.Model(text_input, net)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics='accuracy')\n",
    "       \n",
    "    return model  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fFR3532Uf4Hb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#define TF tuner object with Hyperband\n",
    "TF_tuner = kt.Hyperband(\n",
    "    hypermodel=build_TF_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=20,\n",
    "    factor=3,\n",
    "    directory='tuner_results',\n",
    "    project_name='bert_tuning',\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train.values, y_train.values)).batch(32)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val.values, y_val.values)).batch(32)\n",
    "#model.summary()\n",
    "\n",
    "TF_tuner.search_space_summary()\n",
    "\n",
    "# Define hyperparameters using the instance\n",
    "TF_tuner.search(train_dataset,\n",
    "            epochs=20,\n",
    "            validation_data=val_dataset,\n",
    "            callbacks=[early_stopping])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_Vat4QNjn-7"
   },
   "outputs": [],
   "source": [
    "best_TF_hps=TF_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "#print results\n",
    "print(f\"\"\"\n",
    "Best hyperparameters found:\n",
    "-learning_rate: {best_TF_hps.get('learning_rate')}\n",
    "-dropout_rate: {best_TF_hps.get('dropout_rate')}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CmQ1aI7Sl6y"
   },
   "outputs": [],
   "source": [
    "#plot the transformer model\n",
    "TF_model = build_TF_model(best_TF_hps)\n",
    "\n",
    "tf.keras.utils.plot_model(TF_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S76pI6FpwTvh"
   },
   "source": [
    "###RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "PXWUFALQwYeN",
    "outputId": "fecd4fdb-c09d-4b08-9b8c-1e8222eb9128"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1739440460.127283     619 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2248 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 with Max-Q Design, pci bus id: 0000:02:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "#we are going to define RNN model considering the 20.000 most popular words to keep things maneageble\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(20000, 128))\n",
    "model.add(tf.keras.layers.LSTM(64))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nH2WPDrM73vg"
   },
   "outputs": [],
   "source": [
    "def build_RNN_model(hp):\n",
    "  model = tf.keras.Sequential()\n",
    "  #model.add(Input(shape=(None, 128)))\n",
    "  model.add(tf.keras.layers.Embedding(20000, 128))\n",
    "\n",
    "  num_layers = hp.Int('num_layers', 1, 3, 1)\n",
    "  for i in range(num_layers):\n",
    "\n",
    "    units = hp.Int(f'units_layer_{i+1}', min_value=32, max_value=256, step=32)\n",
    "\n",
    "    #if we are in the last layer we don't return sequences\n",
    "    return_sequences = i<num_layers-1\n",
    "    model.add(tf.keras.layers.LSTM(units=units, return_sequences=True))\n",
    "\n",
    "\n",
    "    #Dropout\n",
    "    if hp.Boolean('dropout_layer_{i+1}'):\n",
    "      model.add(tf.keras.layers.Dropout(rate=hp.Float('dropout_rate_{i+1}', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "    #add bidirectional layer\n",
    "    model.add(Bidirectional(tf.keras.layers.LSTM(units=units, return_sequences=False)))\n",
    "\n",
    "      \n",
    "    #output layer\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid', kernel_regularizer=l2(0.001)))\n",
    "\n",
    "    \n",
    "    #using AdamW\n",
    "    optimizer=tf.keras.optimizers.AdamW(learning_rate=hp.Choice('learning_rate',[1e-5,2e-5,3e-5]), weight_decay=1e-4)\n",
    "\n",
    "    #compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4RCPXixazgj3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizer\u001b[49m(num_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, oov_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<OOV>\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# Initialize tokenizer\u001b[39;00m\n\u001b[1;32m      2\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mfit_on_texts(x_train) \u001b[38;5;66;03m# Fit on training data\u001b[39;00m\n\u001b[1;32m      3\u001b[0m x_train_seq \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(x_train) \u001b[38;5;66;03m# Convert text to sequences\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>') # Initialize tokenizer\n",
    "tokenizer.fit_on_texts(x_train) # Fit on training data\n",
    "x_train_seq = tokenizer.texts_to_sequences(x_train) # Convert text to sequences\n",
    "x_val_seq = tokenizer.texts_to_sequences(x_val) # Convert text to sequences\n",
    "\n",
    "# Pad the sequences to have the same length\n",
    "x_train_padded = pad_sequences(x_train_seq, padding='post', maxlen=max_length) # Pad training data\n",
    "x_val_padded = pad_sequences(x_val_seq, padding='post', maxlen=max_length) # Pad validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUFfIEyF-_A6"
   },
   "outputs": [],
   "source": [
    "#define RNN tuner object with Hyperband\n",
    "RNN_tuner = kt.Hyperband(\n",
    "    build_RNN_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=20,\n",
    "    factor=3,\n",
    "    directory='tuner_results',\n",
    "    project_name='rnn_tuning')\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "\n",
    "#start search\n",
    "RNN_tuner.search(x_train_padded, y_train, epochs=50, validation_data=(x_val_padded, y_val), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04RjqahZA0i0"
   },
   "outputs": [],
   "source": [
    "best_RNN_hps=RNN_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Numero ottimale di layer:\", best_RNN_hps.get(\"num_layers\"))\n",
    "print(\"Migliore learning rate:\", best_RNN_hps.get(\"learning_rate\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CmQ1aI7Sl6y"
   },
   "outputs": [],
   "source": [
    "#plot the model\n",
    "RNN_model = build_RNN_model(best_RNN_hps)\n",
    "\n",
    "\n",
    "\n",
    "tf.keras.utils.plot_model(RNN_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyIUyw7ztfH9"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqRelzTVq654"
   },
   "source": [
    "### Training Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OtLBSfWhcbbS"
   },
   "outputs": [],
   "source": [
    "#best_TF_model = keras.models.load_model('models/best_TF_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUJotfEqtu1W"
   },
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "\n",
    "best_TF_model = TF_tuner.hypermodel.build(best_TF_hps)\n",
    "\n",
    "print(f'Training model with {tfhub_handle_encoder}')\n",
    "\n",
    "history = best_TF_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=100,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_TF_model.save('models/best_TF_model.tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C4JSTz94-iA1"
   },
   "outputs": [],
   "source": [
    "#doing a negative sample evaluation for Transformer model\n",
    "sample_text = ('i hate you')\n",
    "predictions = best_TF_model.predict([sample_text])\n",
    "print(predictions)\n",
    "\n",
    "if predictions > 0.5:\n",
    "  print('Positive')\n",
    "else:\n",
    "  print('Negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTMl3cem_GZJ"
   },
   "outputs": [],
   "source": [
    "#doing a positive sample evaluation for Transformer model\n",
    "sample_text = ('i love you')\n",
    "predictions = best_TF_model.predict([sample_text])\n",
    "print(predictions)\n",
    "\n",
    "if predictions > 0.5:\n",
    "  print('Positive')\n",
    "else:\n",
    "  print('Negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwHZdlLEx45v"
   },
   "source": [
    "### Training RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_RNN_model = keras.models.load_model('models/best_RNN_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kwRKBX6Fx8gf"
   },
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "\n",
    "best_RNN_model = RNN_tuner.hypermodel.build(best_RNN_hps)\n",
    "\n",
    "history = best_RNN_model.fit(x_train_padded, y_train, epochs=100, validation_data=(x_val_padded, y_val), callbacks=[early_stopping, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_RNN_model.save('models/best_RNN_model.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_RpJfv18I5B"
   },
   "outputs": [],
   "source": [
    "#doing a negative sample evaluation for RNN model\n",
    "sample_text = ('i hate you')\n",
    "sample_text_seq = tokenizer.texts_to_sequences([sample_text]) # Tokenize the sample text\n",
    "sample_text_padded = pad_sequences(sample_text_seq, padding='post', maxlen=max_length) # Pad the sequence\n",
    "prediction = best_RNN_model.predict(sample_text_padded) # Make predictions on the padded sequence\n",
    "\n",
    "print(prediction)\n",
    "#print the prediction converting it to 'Positive' or 'Negative'\n",
    "if prediction > 0.5:\n",
    "  print('Positive')\n",
    "else:\n",
    "  print('Negative')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "wfQuKeT799SN",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#doing a positive sample evaluation for RNN model\n",
    "sample_text = ('i love you')\n",
    "sample_text_seq = tokenizer.texts_to_sequences([sample_text]) # Tokenize the sample text\n",
    "sample_text_padded = pad_sequences(sample_text_seq, padding='post', maxlen=max_length) # Pad the sequence\n",
    "prediction = best_RNN_model.predict(sample_text_padded) # Make predictions on the padded sequence\n",
    "\n",
    "print(prediction)\n",
    "#print the prediction converting it to 'Positive' or 'Negative'\n",
    "if prediction > 0.5:\n",
    "  print('Positive')\n",
    "else:\n",
    "  print('Negative')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
